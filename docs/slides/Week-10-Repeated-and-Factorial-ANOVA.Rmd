---
title: "Repeated measures and factorial ANOVA"
date: "14/12/2021"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "xaringan-themer.css", "css/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "js/macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
xaringanExtra::use_tile_view()
xaringanExtra::use_extra_styles()
knitr::opts_chunk$set(fig.asp = .618,
                      fig.width = 6)
library(tidyverse)
library(afex)
library(emmeans)
library(copula)
library(car)
crime <- read_csv("data/crime.csv")
set.seed(104)
within_cop <- mvdc(normalCopula(param = 0.3, dim = 3),
                   c("norm", "norm", "norm"),
                   paramMargins = list(list(mean = 8, sd = 2),
                                       list(mean = 7.75, sd = 2),
                                       list(mean = 5, sd = 3)))
test_within <- rMvdc(50, within_cop)
```
```{r xaringantheme, include = FALSE, warning = FALSE}
library(xaringanthemer)
style_mono_light(base_color = "#23395b",
                 base_font_size = "21px",
                 link_color = "#C80039")
```

# Multiple linear regression

.pull-left[
Multiple regression is what we need with multiple predictors.
```{r multi-reg-four, eval = FALSE}
a <- 2    # Our intercept term
b1 <- 0.65  # Our first regression coefficient
X1 <- rnorm(1000, 6, 1) # Our first predictor 
b2 <- -0.8 # Our second regression coefficient
X2 <- rnorm(1000, 3, 1) # Our second predictor
err <- rnorm(1000, 0, 1) # Our error term
y <- a + b1 * X1 + b2 * X2 + err # Our response variable
```
]
.pull-right[
A simple regression has one predictor...
```{r eval = FALSE}
lm(y ~ X1)
```

and adding predictors is easy - we use the `+` symbol!
```{r eval = FALSE}
lm(y ~ X1 + X2)
```


]

---
# Comparing three or more means with ANOVA

The `t.test()` can only handle two groups. 

When we have three or more groups, we need to use a One-Way Analysis of Variance (ANOVA).

```{r hypothetical-norms, echo = FALSE, fig.height = 5}
tibble(iq = 0:200,
       group_a = dnorm(iq, mean = 100, sd = 15),
       group_b = dnorm(iq, mean = 110, sd = 15),
       group_c = dnorm(iq, mean = 70, sd = 15)) %>%
  gather(group, density, -iq) %>%
  ggplot(aes(x = iq, y = density)) +
  geom_line(aes(group = group, linetype = group)) +
  theme_classic() 
```

---
# How to run ANOVA with the `afex` package

Although the standard R function for ANOVA, `aov()`, works, it can be fiddly to use.

The `afex` package provides several easier methods for running ANOVA.

We'll use the `aov_ez()` function.
```{r echo = FALSE}
noise_test <- 
  gather(tibble(none = rnorm(50, 7.5, 1.5),
                quiet = rnorm(50, 6.5, 1.5),
                loud = rnorm(50, 5, 1.5)),
         noise, test_score) %>%
  mutate(participant = 1:150)
```
```{r}
noise_aov <- aov_ez(dv = "test_score",
                    between = "noise",
                    id = "participant",
                    data = noise_test)
```

---
class: inverse, middle, center
# Comparing multiple means with dependent data

---
# Within-subjects ANOVA

When the assumption of *independence* is violated - i.e. participants contribute more than one data point, and contribute to more than one design *cell* - we need to use a *within-subjects* or *repeated-measures* ANOVA.

# A worked example
Our researcher from last week wanted to examine the effect of noisy environments on test performance. She recruited 150 participants and splits them into three groups who took the test with no noise, reasonably quiet noise, or loud noise.

One problem here is the possibility that participants in each group just had different levels of ability. To get round this, she decides to get each participant to sit three tests, each under different levels of noise. Thus, any differences attributed to  noise can't be due to test-taking ability.

---
# Within-subjects ANOVA

.pull-left[

```{r sim-dat-old, echo = FALSE}
noise_test <- 
  gather(tibble(none = rnorm(50, 7.5, 1.5),
                quiet = rnorm(50, 6.5, 1.5),
                loud = rnorm(50, 5, 1.5)),
         noise, test_score) %>%
  mutate(participant = 1:150)
```

```{r}
head(noise_test)
```

]
.pull-right[
.large[
Last time we simulated data with a *between-subjects* structure:

One row per observation, which meant one row per participant.
]
]

---
# Within-subjects ANOVA

.pull-left[

```{r echo = FALSE}
noise_test_within <- 
  gather(tibble(none = test_within[, 1],
                quiet = test_within[, 2],#rnorm(50, 7, 1),
                loud = test_within[, 3]),#rnorm(50, 5, 1)),
         noise, test_score) %>%
  mutate(participant = rep(1:50, 3))
```
.large[
This time, it's the same participants in each condition. There's still one row per observation, but now there are three rows per participant - one for each observation in each of the three conditions.
]
]
.pull-right[
```{r}
arrange(noise_test_within, participant)
```
]

---
# Plotting the data

.pull-left[
```{r indiv-within, echo = FALSE}
ggplot(noise_test_within, 
       aes(x = fct_relevel(noise,
                           c("none",
                             "quiet",
                             "loud")),
           y = test_score)) +
  geom_point(alpha = 0.5) + 
  geom_line(aes(group = participant), 
            alpha = 0.3) +
  theme_classic(base_size = 18) +
  labs(x = "Noise level", 
       y = "Test score")
```
]
.pull-right[
.large[
It looks like loud noise has a really detrimental effect on performance; it also looks like loud noise makes performance more *variable*.

```{r}
noise_test_within %>%
  group_by(noise) %>%
  summarise(variance = var(test_score))
```
]
]

---
# Plotting the data

.pull-left[
![](`r knitr::fig_chunk("indiv-within", "png")`)
]
.pull-right[

Furthermore, scores tend to be positively correlated (albeit weakly) across tests - people who score well in one situation tend to score well in other situations.

```{r}
noise_test_within %>%
  pivot_wider(names_from = "noise",
              values_from = "test_score") %>%
  select(2:4) %>% cor() %>% round(2)
```
]

---
# The mean as a model (again)

.pull-left[
```{r echo = FALSE,fig.height=5}
ggplot(noise_test_within,
       aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point() + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  theme_classic(base_size = 18) + 
  facet_wrap(~noise)
```
]

.pull-right[
.large[
The grand mean test score is `r round(mean(noise_test$test_score), 2)`, shown by the black line.

The total variability in our data is the sum of the squared differences from the grand mean - the Total Sum of Squares, $SS_t$.
]
]

---
# The group means as a model

.pull-left[
```{r echo = FALSE,fig.height=5, warning = FALSE}
noise_test_within %>%
  group_by(noise) %>%
  mutate(group_mean = mean(test_score)) %>%
ggplot(aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point(size = 1.5) + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  stat_summary(fun = mean,
               geom = "hline",
               aes(yintercept = group_mean),
               size = 2) +
  geom_hline(aes(yintercept = mean(group_mean)), size = 2) +
  theme_classic(base_size = 18) + 
  facet_wrap(~noise)
```
]

.pull-right[
.large[
Our Model Sum of Squares - $SS_m$ - is the sum of the squared differences of each group's mean from the *grand mean*.

The group means are shown here using coloured lines.

This is just the same as it is for a between-subjects ANOVA.

But the next step is different!
]
]

---
# Within-subject variability

.pull-left[
```{r echo = FALSE,fig.height=5, warning = FALSE}
noise_test_within %>%
  filter(participant <= 10) %>%
  mutate(participant = as.factor(participant)) %>%
ggplot(aes(x = participant,
           y = test_score)) +
  geom_point(size = 4, aes(colour = noise)) + 
  stat_summary(fun = mean,
               geom = "point",
               shape = "triangle",
               size = 5,
               alpha = 0.5) + 
  theme_classic(base_size = 18) 
```
]
.pull-right[
.large[Here,  we need the within-participant sum of squares - $SS_w$. This is the sum of squared differences of each participant's scores from their individual mean.

Each participant's mean is marked using a triangle, while scores from individual conditions are marked with points.]
]
---
# The leftovers, the mean squares, and the F-ratio

Finally, we can calculate the Residual sum of squares - $SS_r$ by subtracting the model sum of squares - $SS_m$ - from the within-subjects sum of squares - $SS_w$.

We then calculate the Model Mean Square Error - $MS_m$ - and Residual Mean Square Error - $MS_r$ - the same way as last time, using the degrees of freedom - 

$$MS_m = \frac{SS_m}{df_m}$$

$$MS_r = \frac{SS_r}{df_r}$$

And we calculate the *F-ratio* in the same way as last time.

$$F = \frac{MS_m}{MS_r}$$

---
# Between- versus within-subject ANOVA

.large[
1.  The underlying computations are mostly the same, but differ in how they treat the variability

2.  Within-subject designs use within-subject variability
    - Within-subject variability is often much lower than between-subject variability
    - People function as their own controls!

3.  Since the variance within-subjects is generally lower than between-subjects, within-subject designs typically have more *statistical power* i.e. are more *sensitive*.

4.  However, there is a risk of *order* or *practice* effects with within-subject designs.
]

---
class: inverse, center, middle
# How to run a one-way within-subjects ANOVA 

---
# Within-subjects ANOVA with `afex`

Just like last week, we can use `aov_ez()` from the `afex` package.

Instead of passing a parameter called *between*, we pass one called *within*.

```{r highlight.output = 5}
noise_within_aov <- 
  aov_ez(dv = "test_score",
         id = "participant",
         within = "noise",
         data = noise_test_within)
noise_within_aov
```

---
# The sphericity assumption

*Sphericity* is the equivalent to the homogeneity of variance assumption when there are three or more levels of a repeated measures factor.

`afex` applies Greenhouse-Geisser correction *by default* - it adapts the *degrees of freedom* to compensate for different variances.

```{r highlight.output = c(5, 9)}
noise_within_aov
```

---
# Effect size

*ges* in the output stands for **Generalized eta-squared** - $\eta_g^2$

This tells us the proportion of variance explained, *similar* to $r^2$.

```{r highlight.output = 5}
noise_within_aov
```

---
# Within-subjects ANOVA

.large[
We can follow up the significant effect in the same way as last time:

```{r}
pairs(emmeans(noise_within_aov, ~noise))
```

Performance in the **quiet** and **no noise** conditions is significantly better than performance in the **Loud noise** condition, but they aren't significantly different from each other.
]


---
# Reporting the results 

```{r}
noise_within_aov
```

.large[
"There was a significant effect of noise level on test scores, [F(1.81, 88.54) = 26.54, *p* < .001]. Test performance without noise and with quiet noise did not significantly differ (*p* = .5), but both were significantly better than performance in the loud noise condition (*p*s < .001)." 
]

---
class: inverse, middle, center
# Comparing multiple means with multiple categorical predictors

---
# Factorial ANOVA
.pull-left[
.large[
Our researcher now wonders whether the level of noise matters more for tests that are hard compared to tests that are relatively easy.

So she runs the study again, with the same three noise conditions, but now splits the participants into two more conditions. Half of the participants take an easy test; the other half take a hard test.
]
]
.pull-right[
```{r echo = FALSE}
within_hard <- mvdc(normalCopula(param = 0.3, dim = 3),
                   c("norm", "norm", "norm"),
                   paramMargins = list(list(mean = 6, sd = 2),
                                       list(mean = 5, sd = 2),
                                       list(mean = 4, sd = 2)))
within_easy <- mvdc(normalCopula(param = 0.3, dim = 3),
                   c("norm", "norm", "norm"),
                   paramMargins = list(list(mean = 7.5, sd = 1),
                                       list(mean = 7.5, sd = 1),
                                       list(mean = 7.5, sd = 2)))
hard_within <- rMvdc(50, within_hard)
easy_within <- rMvdc(50, within_easy)
noise_test_mixed <- 
  gather(tibble(none_hard = hard_within[, 1],
                quiet_hard = hard_within[, 2],
                loud_hard = hard_within[, 3],
                none_easy = easy_within[, 1],
                quiet_easy = easy_within[, 2],
                loud_easy = easy_within[, 3]),
         noise, test_score) %>%
  separate(noise, c("noise", "difficulty")) %>%
  mutate(participant = c(rep(1:50, 3),
                         rep(51:100, 3)))
```
```{r}
noise_test_mixed
```
]

---
# What design does the researcher have?

.large[
Factorial designs can be purely within-subjects, purely between-subjects, or a mixture of the two. There can be any number of factors with any number of levels.

The resulting experiment has **two** independent, categorical variables, and thus two *factors*.

The factor "test difficulty" has two levels - "easy" and "hard". It is a *between-subjects* factor.

The factor "noise" has three levels - "none", "quiet", and "loud". It is a *within-subjects* factor.

This calls for a Two-Way, $2 \times 3$ Mixed ANOVA.
]

---
# The structure of the data

.pull-left[
Since each participant takes part in all three noise conditions, there are three rows per participant.
```{r}
noise_test_mixed %>%
  arrange(participant)
```
]
.pull-right[
But since difficulty is between-subjects, the participant ID numbers differ across easy and hard difficulties.
```{r}
noise_test_mixed %>%
  group_by(difficulty) %>%
  slice(1:3)
```
]

---
# Mixed factorial ANOVA with `afex`

.large[
We enter *between* factors using the **between** argument; *within* factors using the **within** argument.
]

```{r}
noise_aov_mixed <- aov_ez(id = "participant",
                          dv = "test_score", 
                          between = "difficulty",
                          within = "noise",
                          data = noise_test_mixed)
```

---
# Mixed factorial ANOVA with `afex`

```{r highlight.output = c(5, 6, 7)}
noise_aov_mixed
```

.large[
Looks like **everything** is significant!
]

---
# A worked example

.pull-left[

Here's a similar plot to the one I produced earlier, but using `afex_plot()` instead of `ggplot()`.

```{r afx-plot, fig.show = "hide", fig.height = 5}
afex_plot(noise_aov_mixed,
          x = "noise",
          trace = "difficulty", 
          error = "between") +
  theme_classic()
```

It seems pretty obvious from this plot that there's an effect of noise when the test is hard, but not so much when the test is easy. This is an **interaction** effect.
]
.pull-right[
![](`r knitr::fig_chunk("afx-plot", "png")`)
]

---
# Post-hoc tests

We need to follow up a significant interaction to work out, statistically, what is driving the interaction.

One way to do this is with post-hoc tests. With post-hoc tests, we compare every possible pair of means to each other using t-tests.

First let's get all the means using `emmeans()`.

```{r}
all_means <- emmeans(noise_aov_mixed,
                     ~noise * difficulty)
all_means
```

---
# Post-hoc tests

Then we use the `pairs()` function to test them all against each other.

```{r}
pairs(all_means)
```

---
# Post-hoc tests

.large[
1. Should only be used following a *significant* interaction.

2. Leads to a lot of comparisons - N(N-1) / 2, where N is the number of means. So it's **extremely important** to correct for multiple comparisons!
    - Numerous methods exist; fortunately, `emmeans()` and `pairs()` handle this for us using Tukey's Honestly Significant Difference (HSD). 

3. Can be difficult to interpret, especially with more than two factors. If in doubt, *look at the plots*.
]

---
# Simple effects

An alternative way is with *simple effects*. We can effectively run separate analyses at different levels of one of the factors. Here I look at the means for each level of noise separately for the two levels of difficulty.

```{r}
emmeans(noise_aov_mixed,
        "noise",
        by = "difficulty")
```

---
# Simple effects

Now we run post-hoc tests separately within each level of difficulty.

```{r}
pairs(emmeans(noise_aov_mixed,
        "noise",
        by = "difficulty"))
```

---
# Simple effects
.large[
1.  Often easier to interpret (especially when there are more than two factors).

2.  Fewer comparisons so less stringent correction for multiple comparison, and higher power to detect differences.

3.  Not always obvious which factor to separate on. Sometimes it's easier to interpret one way than the other! Again, *use plots as a guide*.
]

---
# What about the main effects?

I skipped straight to the interaction earlier. Why?

```{r highlight.output = c(5, 6)}
noise_aov_mixed
```

There are significant main effects of *noise* and *difficulty*.

---
# The main effect of noise

There's a significant main effect of noise. Let's look at the plot.

```{r fig.height = 5, warning = FALSE, highlight.output = 1}
afex_plot(noise_aov_mixed,
          ~noise, error = "within") + theme_classic()
```


---
# The main effect of noise

As noise increases, test performance goes down.
```{r}
emmeans(noise_aov_mixed, ~noise)
```

---
# The main effect of difficulty

Let's look at the plot for difficulty.

```{r fig.height = 5}
afex_plot(noise_aov_mixed, ~difficulty, error = "between") + theme_bw()
```

---
# The main effect of difficulty

Test performance is much higher when the test is easy than when it's hard.

```{r}
emmeans(noise_aov_mixed, ~difficulty)
```

---
# Are these effects meaningful?

.pull-left[
```{r inter-plot-again, fig.height = 5, fig.show = "hide"}
afex_plot(noise_aov_mixed,
          ~noise,
          trace = "difficulty") +
  theme_bw()
```

There is clearly no significant effect of noise when the test is easy, so the main effect of noise is uninterpretable.

But there is *always* an effect of test difficulty, so the main effect of test difficulty *is* interpretable.

**Main effects are not always interpretable in the presence of an interaction!**
]
.pull-right[
![](`r knitr::fig_chunk("inter-plot-again", "png")`)
]


---
class: inverse, middle, center
# Reporting factorial ANOVA results

---
# Reporting factorial ANOVAs

.large[
Make sure that somewhere in your text is a description of which type of ANOVA you are running, and exactly what factors are involved. A couple of examples:

1.  We conducted a two-way repeated measures ANOVA with the factors Noise (None, Quiet, or Loud) and Difficulty (Easy or Hard).

2.  We conducted a $2 \times 3$ mixed ANOVA. The between-subjects factor was Difficulty (Easy or Hard), while Noise was a repeated-measures factor (None, Quiet, or Loud)
]

---
# Reporting factorial ANOVAs

```{r echo = FALSE, highlight.output = 5}
noise_aov_mixed
```

There was a significant main effect of Difficulty [F(1, 98) = 98.54, p < .001], with better test performance when the test was easy (mean = 7.62) compared to when the test was hard (mean = 5.15).

*NB: since there are only two levels, no post-hoc test is necessary!*

---
# Reporting factorial ANOVAs

```{r echo = FALSE, highlight.output = 6}
noise_aov_mixed
```

There was a significant main effect of noise [F(1.98, 194.13) = 15.08, *p* < .001]. Test scores were significantly lower with loud noise (5.79) than with no noise (mean = 6.91; *p* < .001) or with quiet noise (6.44; *p* = .005). There was no significant difference between the "no noise" and "quiet noise" conditions (*p* =.06).

---
# Reporting factorial ANOVAs

```{r echo = FALSE, highlight.output = 7}
noise_aov_mixed
```

Finally, there was also a significant interaction between Noise and Difficulty [F(1.98, 194.13) = 13.09, p < .001], see Figure X. Simple main effects analysis, corrected for multiple comparisons using Tukey's HSD, found that when test difficulty was Easy, there were no significant differences between any level of Noise (all *p*s > .58). However, in the Hard condition, test performance was significantly better when there was no noise (mean = 6.25) compared to when there was either quiet (5.11; *p* < .001) or loud noise (4.08, *p* < .001). Performance was also significantly worse for loud noise relative to quiet noise (*p* = .002).
